{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Working_with_DF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjuAVgrxmIo3"
      },
      "source": [
        "# **Working with DF (DataFrame)**\n",
        "\n",
        "**`Udemy Course: Best Hands-on Big Data Practices and Use Cases using PySpark`**\n",
        "\n",
        "**`Author: Amin Karami (PhD, FHEA)`**\n",
        "\n",
        "---\n",
        "\n",
        "**DataFrame (DF)**: Schema (named columns) + declarative language. A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. It is very efficient for strucutred data.\n",
        "\n",
        "source: https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
        "\n",
        "source: https://spark.apache.org/docs/latest/api/python/reference/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LWTJaC8mHL5"
      },
      "source": [
        "########## ONLY in Colab ##########\n",
        "!pip3 install pyspark\n",
        "########## ONLY in Colab ##########"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########## ONLY in Ubuntu Machine ##########\n",
        "# Load Spark engine\n",
        "!pip3 install -q findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "########## ONLY in Ubuntu Machine ##########"
      ],
      "metadata": {
        "id": "K-riUQ6WTHDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linking with Spark (https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html)\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "id": "e3pTfRiwTMeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Create DF and Basic Operations**"
      ],
      "metadata": {
        "id": "quQ_GBpgWLRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create/Load DF: (Spark automatically scans through the files and infers the schema of the dataset)\n",
        "# data source: https://www.kaggle.com/thec03u5/fifa-18-demo-player-dataset\n",
        "\n",
        "df1 = spark.read.format(\"csv\").load(\"CompleteDataset.csv\", inferSchema=True, header=True)"
      ],
      "metadata": {
        "id": "1n39Bv24XHjt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show data:\n",
        "df1.show()"
      ],
      "metadata": {
        "id": "b8aOYoMLX7Er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many partitions in DF?\n",
        "df1.rdd.getNumPartitions()"
      ],
      "metadata": {
        "id": "9EffFOyTYC18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Increase/Desrease the partitions in DF\n",
        "df2 = df1.repartition(4)\n",
        "df2.rdd.getNumPartitions()"
      ],
      "metadata": {
        "id": "YBP0nzanB1Cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show DF\n",
        "df2.show()"
      ],
      "metadata": {
        "id": "N0sR-ffdCKuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename Columns and Amend NULLs:\n",
        "df2 = df2.withColumnRenamed(\"_c0\", \"ID\") \\\n",
        "    .withColumnRenamed(\"Ball control\", \"Ball_Control\")\\\n",
        "    .withColumnRenamed(\"Sliding tackle\", \"Sliding_Tackle\")\n",
        "\n",
        "df2.na.fill({\"RAM\": 10, \"RB\": 1}).show()"
      ],
      "metadata": {
        "id": "Yp9QWyKcDQZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformation (SELECT):\n",
        "df2.select(\"Name\",\"Overall\").distinct().show()"
      ],
      "metadata": {
        "id": "Vf9eiG93HbKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformation (FILTER):\n",
        "df2.filter(df2[\"Overall\"] > 70).show()"
      ],
      "metadata": {
        "id": "QlLKM3krHyWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformation (FILTER):\n",
        "df2.select(\"Overall\", \"Name\", \"Age\").where(df2[\"Overall\"]>70).show()"
      ],
      "metadata": {
        "id": "7b94hjJuIDj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformation (FILTER):\n",
        "df2.where(df2[\"Overall\"]>70).groupBy(\"Age\").count().sort(\"Age\").show()"
      ],
      "metadata": {
        "id": "vvnmUbd1IgKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the results:\n",
        "df2_result = df2.where(df2[\"Overall\"]>70).groupBy(\"Age\").count().sort(\"Age\")\n",
        "\n",
        "pandas_df = df2_result.toPandas()\n",
        "pandas_df.plot(x = \"Age\", y = \"count\", kind = \"bar\")\n"
      ],
      "metadata": {
        "id": "qyoxZNRDJYJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pandas_df.sort_values(by=\"count\", ascending=False).plot(x = \"Age\", y = \"count\", kind = \"bar\")"
      ],
      "metadata": {
        "id": "xL4ac6O7wZvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: Advanced DF Operations: Spark SQL and UDF**"
      ],
      "metadata": {
        "id": "EGi2zdncaoHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark SQL (Register the DF using a local temporary view):\n",
        "\n",
        "df2.createOrReplaceTempView(\"df_football\")"
      ],
      "metadata": {
        "id": "YhJzLq8XaoCL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SQL Query:\n",
        "\n",
        "sql_query = \"\"\" SELECT Age, count(*) as Count\n",
        "                FROM df_football\n",
        "                WHERE Overall > 70\n",
        "                GROUP BY Age\n",
        "                ORDER BY Age \"\"\"\n",
        "\n",
        "result = spark.sql(sql_query)\n",
        "result.show()"
      ],
      "metadata": {
        "id": "bIKu4KMrdt1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UDF (User Defined Functions):\n",
        "def uppercase_converter(record):\n",
        "  if len(record) > 10:\n",
        "    return record.upper()\n",
        "  else:\n",
        "    return record.lower()\n",
        "\n",
        "# register the DF\n",
        "df2.createOrReplaceTempView(\"UDF_football\")\n",
        "\n",
        "# register the function\n",
        "spark.udf.register(\"UPPER\", uppercase_converter)\n",
        "\n",
        "# use the UDF in SQL\n",
        "sql_query = \"SELECT Age, UPPER(Name) as Name, UPPER(Club) as Club FROM UDF_football\"\n",
        "\n",
        "result = spark.sql(sql_query)\n",
        "result.show()\n"
      ],
      "metadata": {
        "id": "OgP3auupaaTp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}